{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    v_root = Path(__file__).parents[2]\n",
    "    sys.path.append(os.path.abspath(v_root))\n",
    "except Exception as e:\n",
    "    print(\"erro: \", e)\n",
    "    raise\n",
    "\n",
    "from igem.epc import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Path\n",
    "v_load = v_root / \"_utils\" / \"jiayan_analysis\" / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGA DOS DADOS\n",
    "# dados originados do WORD TO TERM\n",
    "normalization = load.from_csv(str(v_load / \"Normalization.csv\"))\n",
    "# consulta dos TERM encontrados na TERMMAP, para Term_1 e Term_2\n",
    "keylink = load.from_csv(str(v_load / \"keylink.csv\"))\n",
    "# ID and Desc NHAMES\n",
    "VarDescription = load.from_csv(str(v_load / \"VarDescription.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOLAR COLUNAS DE DADOS TARGETS\n",
    "# Process to find pairs\n",
    "pairID = keylink[[\"keyge_1\", \"keyge_2\"]].copy()\n",
    "normalization_short = normalization[[\"Fatores\", \"keyge\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELIMINAR DADOS QUE NAO SAO TARGETS\n",
    "for x in [\"anat\", \"go\", \"path\", \"meta:hmdb0002111\"]:\n",
    "    normalization_short = normalization_short[\n",
    "        ~normalization_short[\"keyge\"].astype(str).str.startswith(x)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELIMINAR DADOS INCONSISTENTES\n",
    "normalization_clean = normalization_short.dropna()\n",
    "# aqui o Jiayan eliminou os fatores duplicados, eliminando aqui TERMS\n",
    "normalization_clean = normalization_clean.drop_duplicates(subset=\"Fatores\", keep=\"last\")\n",
    "\n",
    "print(normalization_clean)\n",
    "print(pairID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE TABLES:\n",
    "# RETORNA AS LIGACOES EM QUE TEMOS TANTO KEYGE1 QUANTO KEYGE2 NA NORMALIZATIONS_CLEAN\n",
    "pairMap = pairID.merge(\n",
    "    normalization_clean, left_on=\"keyge_1\", right_on=\"keyge\", how=\"left\"\n",
    ")\n",
    "pairMap = pairMap.merge(\n",
    "    normalization_clean, left_on=\"keyge_2\", right_on=\"keyge\", how=\"left\"\n",
    ")\n",
    "normalization_short = normalization[[\"Fatores\", \"keyge\"]].copy()\n",
    "pairMap = pairMap.dropna()\n",
    "pairMap2 = pairMap[[\"keyge_1\", \"keyge_2\", \"Fatores_x\", \"Fatores_y\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VarDesc_Short = VarDescription[[\"var\", \"var_desc\"]].copy()\n",
    "VarDesc_clean = VarDesc_Short.drop_duplicates(subset=\"var_desc\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairMap2[\"Fatores_x\"] = pairMap2[\"Fatores_x\"].str.upper()\n",
    "pairMap2[\"Fatores_y\"] = pairMap2[\"Fatores_y\"].str.upper()\n",
    "VarDesc_clean[\"var_desc\"] = VarDesc_clean[\"var_desc\"].str.upper()\n",
    "\n",
    "ToNAHNESID = pairMap2.merge(\n",
    "    VarDesc_clean, left_on=\"Fatores_x\", right_on=\"var_desc\", how=\"left\"\n",
    ")\n",
    "ToNAHNESID = ToNAHNESID.merge(\n",
    "    VarDesc_clean, left_on=\"Fatores_y\", right_on=\"var_desc\", how=\"left\"\n",
    ")\n",
    "\n",
    "NAHNESID = ToNAHNESID[[\"var_desc_x\", \"var_desc_y\"]]\n",
    "\n",
    "NAHNESID = NAHNESID.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gepairs = NAHNESID  # or read the file\n",
    "MainTable = load.from_csv(str(v_load / \"MainTable.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [\n",
    "    \"pneu\",\n",
    "    \"current_asthma\",\n",
    "    \"EVER\",\n",
    "    \"any\",\n",
    "    \"ATORVASTATIN\",\n",
    "    \"AZITHROMYCIN\",\n",
    "    \"CARVEDILOL\",\n",
    "    \"hepb\",\n",
    "    \"FENOFIBRATE\",\n",
    "    \"FLUOXETINE\",\n",
    "    \"BUPROPION\",\n",
    "    \"GLYBURIDE\",\n",
    "    \"ASPIRIN\",\n",
    "    \"heroin\",\n",
    "    \"ALENDRONATE\",\n",
    "    \"METFORMIN\",\n",
    "    \"ESTRADIOL\",\n",
    "    \"OMEPRAZOLE\",\n",
    "    \"NIFEDIPINE\",\n",
    "    \"PREDNISONE\",\n",
    "    \"PIOGLITAZONE\",\n",
    "    \"ROFECOXIB\",\n",
    "    \"ALBUTEROL\",\n",
    "    \"SPIRONOLACTONE\",\n",
    "    \"SIMVASTATIN\",\n",
    "    \"SERTRALINE\",\n",
    "    \"LOVASTATIN\",\n",
    "    \"LOSARTAN\",\n",
    "    \"cocaine\",\n",
    "    \"DIGOXIN\",\n",
    "    \"CELECOXIB\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in remove:\n",
    "    gepairs = gepairs[~gepairs[\"var_desc_x\"].str.contains(i)]\n",
    "    gepairs = gepairs[~gepairs[\"var_desc_y\"].str.contains(i)]\n",
    "\n",
    "gepairs = gepairs.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HEMOGLOBIN\n",
    "resultstable_dis = pd.DataFrame()\n",
    "resultstable_rep = pd.DataFrame()\n",
    "\n",
    "\n",
    "nested_table = MainTable.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"LBXHGB\",\n",
    "        \"female\",\n",
    "        \"black\",\n",
    "        \"mexican\",\n",
    "        \"other_hispanic\",\n",
    "        \"other_eth\",\n",
    "        \"SDDSRVYR\",\n",
    "        \"BMXBMI\",\n",
    "        \"SES_LEVEL\",\n",
    "        \"RIDAGEYR\",\n",
    "        \"LBXCOT\",\n",
    "        \"IRON_mg\",\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(gepairs)):\n",
    "for i in range(1):\n",
    "    e1 = gepairs.loc[i][1]\n",
    "    e2 = gepairs.loc[i][2]\n",
    "    nested_table[\"e1\"] = e1\n",
    "    nested_table[\"e2\"] = e2\n",
    "    nested_table = nested_table.fillna(0)\n",
    "    nested_table_dis = nested_table[nested_table[\"SDDSRVYR\"].isin([1, 2])]\n",
    "\n",
    "    complex_table = nested_table\n",
    "    complex_table[\"interaction\"] = complex_table[\"e1\"] + complex_table[\"e2\"]\n",
    "    complex_table_dis = nested_table[nested_table[\"SDDSRVYR\"].isin([1, 2])]\n",
    "\n",
    "    # Regression\n",
    "    y1 = nested_table[\"LBXHGB\"]\n",
    "    X1 = nested_table[\n",
    "        [\n",
    "            \"female\",\n",
    "            \"black\",\n",
    "            \"mexican\",\n",
    "            \"other_hispanic\",\n",
    "            \"other_eth\",\n",
    "            \"SDDSRVYR\",\n",
    "            \"BMXBMI\",\n",
    "            \"SES_LEVEL\",\n",
    "            \"RIDAGEYR\",\n",
    "            \"LBXCOT\",\n",
    "            \"IRON_mg\",\n",
    "            \"e1\",\n",
    "            \"e2\",\n",
    "        ]\n",
    "    ]\n",
    "    X1 = sm.add_constant(X1)\n",
    "    nested = sm.OLS(y1, X1).fit()\n",
    "    nested_ll = nested.llf\n",
    "    print(nested_ll)\n",
    "\n",
    "    y2 = complex_table[\"LBXHGB\"]\n",
    "    X2 = complex_table[\n",
    "        [\n",
    "            \"female\",\n",
    "            \"black\",\n",
    "            \"mexican\",\n",
    "            \"other_hispanic\",\n",
    "            \"other_eth\",\n",
    "            \"SDDSRVYR\",\n",
    "            \"BMXBMI\",\n",
    "            \"SES_LEVEL\",\n",
    "            \"RIDAGEYR\",\n",
    "            \"LBXCOT\",\n",
    "            \"IRON_mg\",\n",
    "            \"e1\",\n",
    "            \"e2\",\n",
    "            \"interation\",\n",
    "        ]\n",
    "    ]\n",
    "    X2 = sm.add_constant(X2)\n",
    "    complex = sm.OLS(y2, X2).fit()\n",
    "    complex_ll = complex.llf\n",
    "    print(complex_ll)\n",
    "\n",
    "    # TODO: podemos compactar e utilizar apenas uma tabela de dados de entrada\n",
    "\n",
    "    # STEP 3: Perform the Log-Likelihood Test\n",
    "    # Next, weâ€™ll use the following code to perform the log-likelihood test:\n",
    "    # calculate likelihood ratio Chi-Squared test statistic\n",
    "    LR_statistic = -2 * (nested_ll - complex_ll)\n",
    "    print(LR_statistic)\n",
    "\n",
    "    # calculate p-value of test statistic using 2 degrees of freedom\n",
    "    p_val = scipy.stats.chi2.sf(LR_statistic, 2)\n",
    "\n",
    "    print(p_val)\n",
    "\n",
    "    resultstable_dis[i, 1] = p_val\n",
    "    resultstable_dis[i, 2] = e1\n",
    "    resultstable_dis[i, 3] = e2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
